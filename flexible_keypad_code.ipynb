{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 800, 30)\n",
      "(1200, 800, 2)\n",
      "(2, 800, 30)\n",
      "(2, 800, 2)\n",
      "(960000, 30)\n",
      "(960000, 2)\n",
      "(1600, 30)\n",
      "(1600, 2)\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "#파일경로에 한글명이 있으면 안된다\n",
    "name_to_seq = pd.read_csv('D:\\AI_data_set\\Patch\\One_Pad_filename_label.csv', header = None, squeeze = True)\n",
    "\n",
    "train_X = []\n",
    "test_X = []\n",
    "\n",
    "train_Y = []\n",
    "test_Y = []\n",
    "\n",
    "\n",
    "for i in range(0,60):\n",
    "    for k in range(0,20):\n",
    "        filepath = 'D:\\AI_data_set\\Patch\\One_Pad_Model&Data\\One_Pad_1N_csv\\One_Pad_1N_'+ str(name_to_seq[i]) + '_' + str(k+1) + '.csv'\n",
    "        df = np.array(pd.read_csv(filepath, header = None))\n",
    "        Y_press = df[:,2]\n",
    "        Y_category = np.repeat(i, 800)\n",
    "        Y_temp = np.stack((Y_press, Y_category), axis = 1)\n",
    "    \n",
    "        X_temp = df[:,3:33]\n",
    "    \n",
    "        train_X.append(X_temp)\n",
    "        train_Y.append(Y_temp)\n",
    "\n",
    "for k in range(20,22):\n",
    "        filepath = 'D:\\AI_data_set\\Patch\\One_Pad_Model&Data\\One_Pad_1N_csv\\One_Pad_1N_'+ str(name_to_seq[i]) + '_' + str(k+1) + '.csv'\n",
    "        df = np.array(pd.read_csv(filepath, header = None))\n",
    "        Y_press = df[:,2]\n",
    "        Y_category = np.repeat(i, 800)\n",
    "        Y_temp = np.stack((Y_press, Y_category), axis = 1)\n",
    "        X_temp = df[:,3:33]\n",
    "        \n",
    "        test_X.append(X_temp)\n",
    "        test_Y.append(Y_temp)\n",
    "        \n",
    "train_X = np.array(train_X)\n",
    "test_X = np.array(test_X)\n",
    "train_Y = np.array(train_Y)\n",
    "test_Y = np.array(test_Y)\n",
    "\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "\n",
    "test_Y = test_Y.reshape(-1, test_Y.shape[-1])\n",
    "train_Y = train_Y.reshape(-1, train_Y.shape[-1])\n",
    "\n",
    "test_X = test_X.reshape(-1, test_X.shape[-1])\n",
    "train_X = train_X.reshape(-1, train_X.shape[-1])\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "\n",
    "test_press_Y = test_Y[:,0]\n",
    "train_press_Y = train_Y[:,0]\n",
    "test_cate_Y = test_Y[:,1]\n",
    "train_cate_Y = train_Y[:,1]\n",
    "\n",
    "test_cate_Y = to_categorical(test_cate_Y,60)\n",
    "train_cate_Y = to_categorical(train_cate_Y,60)\n",
    "train_press_Y = np.expand_dims(train_press_Y,1)\n",
    "test_press_Y = np.expand_dims(test_press_Y,1)\n",
    "\n",
    "print(test_cate_Y)\n",
    "\n",
    "rand_train = np.arange(len(train_Y))\n",
    "np.random.shuffle(rand_train)\n",
    "r_train_X = train_X[rand_train]\n",
    "r_train_press_Y = train_press_Y[rand_train]\n",
    "r_train_cate_Y = train_cate_Y[rand_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Merge, Concatenate\n",
    "from keras.layers import Dropout, Activation, LeakyReLU, PReLU, ELU, ThresholdedReLU\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, AveragePooling1D, AveragePooling2D\n",
    "from keras.layers import Input, Dense, TimeDistributed, GRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM, Reshape\n",
    "from keras.layers import ZeroPadding2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.utils  import to_categorical\n",
    "import keras.callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 4500)              139500    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 4500)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1500)              6751500   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 150)               225150    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 7,116,301\n",
      "Trainable params: 7,116,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def press_model_set():\n",
    "    \n",
    "    x_input = Input(shape=(30,))\n",
    "    \n",
    "    x = Dense(4500, kernel_initializer = 'glorot_normal', activation='relu')(x_input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(1500, kernel_initializer = 'glorot_normal', activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(150, kernel_initializer = 'glorot_normal', activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    output =  Dense(1, kernel_initializer = 'glorot_normal', activation='linear')(x)    \n",
    "    \n",
    "    # Model set and compile\n",
    "    final_model = Model(inputs = x_input, outputs = output)\n",
    "    \n",
    "    final_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    \n",
    "    return final_model\n",
    "    \n",
    "    \n",
    "press_model = press_model_set()\n",
    "press_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.412265337257358 0.051597371955152335\n",
      "[0.03821206]\n",
      "[[1.         0.98368675]\n",
      " [0.98368675 1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQxL8hRDCD0nwF0IIP/T/AV3uoXrVEPxGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% test naive model \n",
    "%matplotlib inline\n",
    "filepath='D:/AI_data_set/Patch/One_Pad_Model&Data/180118_press_One_Pad.out'\n",
    "\n",
    "press_model.load_weights(filepath)\n",
    "predicted = press_model.predict(test_X)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "print(rmse(predicted, test_press_Y) / abs(np.mean(test_press_Y)) *100,rmse(predicted, test_press_Y))\n",
    "print(sum(abs(predicted- test_press_Y))/(len(test_press_Y)))\n",
    "print(np.corrcoef(np.squeeze(test_press_Y), np.squeeze(predicted)))\n",
    "\n",
    "plt.plot([-1, 0], [-1,0], color='black', linestyle='-', linewidth=2)\n",
    "plt.scatter(test_press_Y,predicted,s=1,color='black')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 4500)              139500    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 4500)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1500)              6751500   \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 300)               450300    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 3)                 903       \n",
      "=================================================================\n",
      "Total params: 7,342,203\n",
      "Trainable params: 7,342,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def press_cate_model_set():\n",
    "    \n",
    "    x_input = Input(shape=(30,))\n",
    "    \n",
    "    x = Dense(4500, kernel_initializer = 'glorot_normal', activation='relu')(x_input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(1500, kernel_initializer = 'glorot_normal', activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(300, kernel_initializer = 'glorot_normal', activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    output =  Dense(3, kernel_initializer = 'glorot_normal', activation='softmax')(x)    \n",
    "    \n",
    "    # Model set and compile\n",
    "    final_model = Model(inputs = x_input, outputs = output)\n",
    "    \n",
    "    final_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return final_model\n",
    "    \n",
    "    \n",
    "press_cate_model = press_cate_model_set()\n",
    "press_cate_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    494160\n",
      "dtype: int64\n",
      "0    239160\n",
      "dtype: int64\n",
      "0    228280\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cut_list =[-1,-0.78,-0.53,0]\n",
    "\n",
    "df = pd.DataFrame(train_press_Y)\n",
    "train_press_cate_Y = df.apply(lambda x : pd.cut(x, cut_list,labels=[2,1,0]))\n",
    "\n",
    "df = pd.DataFrame(test_press_Y)\n",
    "test_press_cate_Y = df.apply(lambda x : pd.cut(x, cut_list,labels=[2,1,0]))\n",
    "\n",
    "print(np.sum(train_press_cate_Y == 0) + np.sum(test_press_cate_Y == 0))\n",
    "print(np.sum(train_press_cate_Y == 1) + np.sum(test_press_cate_Y == 1))\n",
    "print(np.sum(train_press_cate_Y == 2) + np.sum(test_press_cate_Y == 2))\n",
    "\n",
    "\n",
    "test_press_cate_Y = to_categorical(test_press_cate_Y,3)\n",
    "train_press_cate_Y = to_categorical(train_press_cate_Y,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_train = np.arange(len(train_Y))\n",
    "np.random.shuffle(rand_train)\n",
    "r_train_X = train_X[rand_train]\n",
    "\n",
    "r_train_press_cate_Y = train_press_cate_Y[rand_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 0s 122us/step\n",
      "Test score: 0.09625198048472981\n",
      "Test accuracy: 0.961875\n",
      "960000/960000 [==============================] - 81s 85us/step\n",
      "Test score: 0.10379134243722372\n",
      "Test accuracy: 0.9557760416666666\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "filepath='D:/AI_data_set/Patch/One_Pad_Model&Data/180117_One_Pad_press_cate_model_522.out'\n",
    "\n",
    "press_cate_model.load_weights(filepath)\n",
    "score, acc =  press_cate_model.evaluate(test_X, test_press_cate_Y)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "#==============================================================================\n",
    "#Test score: 0.131301972057\n",
    "#Test accuracy: 0.94528125\n",
    "#==============================================================================\n",
    "\n",
    "score, acc =  press_cate_model.evaluate(train_X, train_press_cate_Y)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    554743\n",
      "dtype: int64\n",
      "0    206567\n",
      "dtype: int64\n",
      "0    200290\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cut_list =[-1,-0.8,-0.6,0]\n",
    "\n",
    "df = pd.DataFrame(train_press_Y)\n",
    "train_press_cate_Y = df.apply(lambda x : pd.cut(x, cut_list,labels=[2,1,0]))\n",
    "\n",
    "df = pd.DataFrame(test_press_Y)\n",
    "test_press_cate_Y = df.apply(lambda x : pd.cut(x, cut_list,labels=[2,1,0]))\n",
    "\n",
    "print(np.sum(train_press_cate_Y == 0) + np.sum(test_press_cate_Y == 0))\n",
    "print(np.sum(train_press_cate_Y == 1) + np.sum(test_press_cate_Y == 1))\n",
    "print(np.sum(train_press_cate_Y == 2) + np.sum(test_press_cate_Y == 2))\n",
    "\n",
    "\n",
    "test_press_cate_Y = to_categorical(test_press_cate_Y,3)\n",
    "train_press_cate_Y = to_categorical(train_press_cate_Y,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_train = np.arange(len(train_Y))\n",
    "np.random.shuffle(rand_train)\n",
    "r_train_X = train_X[rand_train]\n",
    "\n",
    "r_train_press_cate_Y = train_press_cate_Y[rand_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 0s 85us/step\n",
      "Test score: 0.08720891150916714\n",
      "Test accuracy: 0.968125\n",
      "960000/960000 [==============================] - 84s 87us/step\n",
      "Test score: 0.08777402638906084\n",
      "Test accuracy: 0.9620833333333333\n"
     ]
    }
   ],
   "source": [
    "filepath='D:/AI_data_set/Patch/One_Pad_Model&Data/180117_One_Pad_press_cate_model_622.out'\n",
    "\n",
    "press_cate_model.load_weights(filepath)\n",
    "score, acc =  press_cate_model.evaluate(test_X, test_press_cate_Y)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "#==============================================================================\n",
    "# Test score: 0.125556136504\n",
    "# Test accuracy: 0.949541666667\n",
    "#==============================================================================\n",
    "\n",
    "score, acc =  press_cate_model.evaluate(train_X, train_press_cate_Y)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut_list =[-1,-0.8,-0.6,0]\n",
    "\n",
    "df = pd.DataFrame(train_press_Y)\n",
    "train_press_cate_Y = df.apply(lambda x : pd.cut(x, cut_list,labels=[2,1,0]))\n",
    "\n",
    "df = pd.DataFrame(test_press_Y)\n",
    "test_press_cate_Y = df.apply(lambda x : pd.cut(x, cut_list,labels=[2,1,0]))\n",
    "\n",
    "group0_test_index = np.where(test_press_cate_Y==0)[0]\n",
    "group1_test_index = np.where(test_press_cate_Y==1)[0]\n",
    "group2_test_index = np.where(test_press_cate_Y==2)[0]\n",
    "\n",
    "group0_train_index = np.where(train_press_cate_Y==0)[0]\n",
    "group1_train_index = np.where(train_press_cate_Y==1)[0]\n",
    "group2_train_index = np.where(train_press_cate_Y==2)[0]\n",
    "\n",
    "group0_test_press_Y = test_press_Y[group0_test_index]\n",
    "group0_test_cate_Y = test_cate_Y[group0_test_index]\n",
    "group0_test_X = test_X[group0_test_index]\n",
    "\n",
    "group1_test_press_Y = test_press_Y[group1_test_index]\n",
    "group1_test_cate_Y = test_cate_Y[group1_test_index]\n",
    "group1_test_X = test_X[group1_test_index]\n",
    "\n",
    "group2_test_press_Y = test_press_Y[group2_test_index]\n",
    "group2_test_cate_Y = test_cate_Y[group2_test_index]\n",
    "group2_test_X = test_X[group2_test_index]\n",
    "\n",
    "\n",
    "group0_train_press_Y = train_press_Y[group0_train_index]\n",
    "group0_train_cate_Y = train_cate_Y[group0_train_index]\n",
    "group0_train_X = train_X[group0_train_index]\n",
    "\n",
    "group1_train_press_Y = train_press_Y[group1_train_index]\n",
    "group1_train_cate_Y = train_cate_Y[group1_train_index]\n",
    "group1_train_X = train_X[group1_train_index]\n",
    "\n",
    "group2_train_press_Y = train_press_Y[group2_train_index]\n",
    "group2_train_cate_Y = train_cate_Y[group2_train_index]\n",
    "group2_train_X = train_X[group2_train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 4500)              139500    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 4500)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1500)              6751500   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 300)               450300    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 60)                18060     \n",
      "=================================================================\n",
      "Total params: 7,359,360\n",
      "Trainable params: 7,359,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cate_model_set():\n",
    "    \n",
    "    x_input = Input(shape=(30,))\n",
    "    \n",
    "    x = Dense(4500, kernel_initializer = 'glorot_normal', activation='relu')(x_input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(1500, kernel_initializer = 'glorot_normal', activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(300, kernel_initializer = 'glorot_normal', activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    output =  Dense(60, kernel_initializer = 'glorot_normal', activation='softmax')(x)    \n",
    "    \n",
    "    # Model set and compile\n",
    "    final_model = Model(inputs = x_input, outputs = output)\n",
    "    \n",
    "    final_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return final_model\n",
    "    \n",
    "    \n",
    "cate_model = cate_model_set()\n",
    "cate_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 0s 114us/step\n",
      "All Test score: 1.3906855286216382\n",
      "All Test accuracy: 0.67625\n",
      "885/885 [==============================] - 0s 90us/step\n",
      "Group 0 Test score: 2.5142298403222347\n",
      "Group 0 Test accuracy: 0.41468926701842057\n",
      "275/275 [==============================] - 0s 120us/step\n",
      "Group 1 Test score: 1.2401547080519941e-05\n",
      "Group 1 Test accuracy: 1.0\n",
      "440/440 [==============================] - 0s 91us/step\n",
      "Group 2 Test score: 1.1920930376163597e-07\n",
      "Group 2 Test accuracy: 1.0\n",
      "960000/960000 [==============================] - 81s 85us/step\n",
      "All Train score: 1.1520497521030348\n",
      "All Train accuracy: 0.703534375\n",
      "553858/553858 [==============================] - 47s 85us/step\n",
      "Group 0 Train score: 1.9916429498849613\n",
      "Group 0 Train accuracy: 0.4879680351281375\n",
      "206292/206292 [==============================] - 17s 84us/step\n",
      "Group 1 Train score: 0.01374818784177084\n",
      "Group 1 Train accuracy: 0.9951476547806023\n",
      "199850/199850 [==============================] - 17s 84us/step\n",
      "Group 2 Train score: 0.0002213613967092535\n",
      "Group 2 Train accuracy: 0.99993495121341\n"
     ]
    }
   ],
   "source": [
    "filepath='D:/AI_data_set/Patch/One_Pad_Model&Data/180118_One_Pad_cate.out'\n",
    "\n",
    "cate_model.load_weights(filepath)\n",
    "\n",
    "score, acc =  cate_model.evaluate(test_X, test_cate_Y)\n",
    "\n",
    "print('All Test score:', score)\n",
    "print('All Test accuracy:', acc)\n",
    "\n",
    "score, acc =  cate_model.evaluate(group0_test_X, group0_test_cate_Y)\n",
    "\n",
    "print('Group 0 Test score:', score)\n",
    "print('Group 0 Test accuracy:', acc)\n",
    "\n",
    "score, acc =  cate_model.evaluate(group1_test_X, group1_test_cate_Y)\n",
    "\n",
    "print('Group 1 Test score:', score)\n",
    "print('Group 1 Test accuracy:', acc)\n",
    "\n",
    "score, acc =  cate_model.evaluate(group2_test_X, group2_test_cate_Y)\n",
    "\n",
    "print('Group 2 Test score:', score)\n",
    "print('Group 2 Test accuracy:', acc)\n",
    "\n",
    "score, acc =  cate_model.evaluate(train_X, train_cate_Y)\n",
    "\n",
    "print('All Train score:', score)\n",
    "print('All Train accuracy:', acc)\n",
    "\n",
    "score, acc =  cate_model.evaluate(group0_train_X, group0_train_cate_Y)\n",
    "\n",
    "print('Group 0 Train score:', score)\n",
    "print('Group 0 Train accuracy:', acc)\n",
    "\n",
    "score, acc =  cate_model.evaluate(group1_train_X, group1_train_cate_Y)\n",
    "\n",
    "print('Group 1 Train score:', score)\n",
    "print('Group 1 Train accuracy:', acc)\n",
    "\n",
    "score, acc =  cate_model.evaluate(group2_train_X, group2_train_cate_Y)\n",
    "\n",
    "print('Group 2 Train score:', score)\n",
    "print('Group 2 Train accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Test\n",
      "9.412265337257358 0.051597371955152335\n",
      "[0.03821206]\n",
      "[[1.         0.98368675]\n",
      " [0.98368675 1.        ]]\n",
      "Group 0 Test\n",
      "20.03119776180452 0.06583691657892127\n",
      "[0.05195946]\n",
      "[[1.         0.88985554]\n",
      " [0.88985554 1.        ]]\n",
      "Group 1 Test\n",
      "3.642824596395973 0.025769779923452866\n",
      "[0.02188594]\n",
      "[[1.         0.96442005]\n",
      " [0.96442005 1.        ]]\n",
      "Group 2 Test\n",
      "2.6290133085653142 0.023403938462187104\n",
      "[0.02076486]\n",
      "[[1.         0.97632504]\n",
      " [0.97632504 1.        ]]\n",
      "All Train\n",
      "10.126452896481151 0.05300675890258339\n",
      "[0.03689051]\n",
      "[[1.         0.97861829]\n",
      " [0.97861829 1.        ]]\n",
      "Group 0 Train\n",
      "20.227970392323396 0.06666088365193364\n",
      "[0.05059598]\n",
      "[[1.         0.88455356]\n",
      " [0.88455356 1.        ]]\n",
      "Group 1 Train\n",
      "3.7217499027312937 0.026592414517973048\n",
      "[0.01972125]\n",
      "[[1.         0.89716989]\n",
      " [0.89716989 1.        ]]\n",
      "Group 2 Train\n",
      "2.461169560541957 0.0212545048203028\n",
      "[0.01663029]\n",
      "[[1.        0.8614587]\n",
      " [0.8614587 1.       ]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "filepath='D:/AI_data_set/Patch/One_Pad_Model&Data/180118_press_One_Pad.out'\n",
    "press_model.load_weights(filepath)\n",
    "\n",
    "predicted = press_model.predict(test_X)\n",
    "true_label = test_press_Y\n",
    "\n",
    "print(\"All Test\")\n",
    "print(rmse(predicted, true_label) / abs(np.mean(true_label)) *100,rmse(predicted, true_label))\n",
    "print(sum(abs(predicted- true_label))/(len(true_label)))\n",
    "print(np.corrcoef(np.squeeze(true_label), np.squeeze(predicted)))\n",
    "\n",
    "predicted = press_model.predict(group0_test_X)\n",
    "true_label = group0_test_press_Y\n",
    "\n",
    "print(\"Group 0 Test\")\n",
    "print(rmse(predicted, true_label) / abs(np.mean(true_label)) *100,rmse(predicted, true_label))\n",
    "print(sum(abs(predicted- true_label))/(len(true_label)))\n",
    "print(np.corrcoef(np.squeeze(true_label), np.squeeze(predicted)))\n",
    "\n",
    "predicted = press_model.predict(group1_test_X)\n",
    "true_label = group1_test_press_Y\n",
    "\n",
    "print(\"Group 1 Test\")\n",
    "print(rmse(predicted, true_label) / abs(np.mean(true_label)) *100,rmse(predicted, true_label))\n",
    "print(sum(abs(predicted- true_label))/(len(true_label)))\n",
    "print(np.corrcoef(np.squeeze(true_label), np.squeeze(predicted)))\n",
    "\n",
    "predicted = press_model.predict(group2_test_X)\n",
    "true_label = group2_test_press_Y\n",
    "\n",
    "print(\"Group 2 Test\")\n",
    "print(rmse(predicted, true_label) / abs(np.mean(true_label)) *100,rmse(predicted, true_label))\n",
    "print(sum(abs(predicted- true_label))/(len(true_label)))\n",
    "print(np.corrcoef(np.squeeze(true_label), np.squeeze(predicted)))\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "predicted = press_model.predict(train_X)\n",
    "true_label = train_press_Y\n",
    "\n",
    "print(\"All Train\")\n",
    "print(rmse(predicted, true_label) / abs(np.mean(true_label)) *100,rmse(predicted, true_label))\n",
    "print(sum(abs(predicted- true_label))/(len(true_label)))\n",
    "print(np.corrcoef(np.squeeze(true_label), np.squeeze(predicted)))\n",
    "\n",
    "predicted = press_model.predict(group0_train_X)\n",
    "true_label = group0_train_press_Y\n",
    "\n",
    "print(\"Group 0 Train\")\n",
    "print(rmse(predicted, true_label) / abs(np.mean(true_label)) *100,rmse(predicted, true_label))\n",
    "print(sum(abs(predicted- true_label))/(len(true_label)))\n",
    "print(np.corrcoef(np.squeeze(true_label), np.squeeze(predicted)))\n",
    "\n",
    "predicted = press_model.predict(group1_train_X)\n",
    "true_label = group1_train_press_Y\n",
    "\n",
    "print(\"Group 1 Train\")\n",
    "print(rmse(predicted, true_label) / abs(np.mean(true_label)) *100,rmse(predicted, true_label))\n",
    "print(sum(abs(predicted- true_label))/(len(true_label)))\n",
    "print(np.corrcoef(np.squeeze(true_label), np.squeeze(predicted)))\n",
    "\n",
    "predicted = press_model.predict(group2_train_X)\n",
    "true_label = group2_train_press_Y\n",
    "\n",
    "print(\"Group 2 Train\")\n",
    "print(rmse(predicted, true_label) / abs(np.mean(true_label)) *100,rmse(predicted, true_label))\n",
    "print(sum(abs(predicted- true_label))/(len(true_label)))\n",
    "print(np.corrcoef(np.squeeze(true_label), np.squeeze(predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0RNcWjQ8a2QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted = press_model.predict(test_X)\n",
    "true_label = test_press_Y\n",
    "\n",
    "plt.plot([-1, 0], [-1,0], color='black', linestyle='-', linewidth=2)\n",
    "plt.axvline(x=-0.8, color='red')\n",
    "plt.axvline(x=-0.6, color='red')\n",
    "plt.scatter(test_press_Y,predicted,s=1,color='black')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0.63375, 0.37791932059447986, 1.0, 1.0, 59, False, 59, False, 59, False, 59, False)]\n",
      "0\n",
      "[(0, 0, 0.63375, 0.37791932059447986, 1.0, 1.0, 59, False, 59, False, 59, False, 59, False), (0, 1, 0.71875, 0.4565217437375571, 1.0, 1.0, 59, False, 59, False, 59, False, 59, False)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-d14497343c37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mgroup2_sep_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msep_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup2_sep_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_all\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mcate_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msep_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_0\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mcate_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup0_sep_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup0_sep_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_1\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mcate_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup1_sep_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup1_sep_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "cut_list =[-1,-0.8,-0.6,0]\n",
    "filepath='D:/AI_data_set/Patch/One_Pad_Model&Data/180118_One_Pad_cate.out'\n",
    "cate_model.load_weights(filepath) \n",
    "\n",
    "number_of_group =2\n",
    "number_of_label =60\n",
    "\n",
    "for i in range(0, number_of_group*number_of_label):\n",
    "    \n",
    "    start_num = 800*(i)\n",
    "    end_num = 800*(i+1)\n",
    "    sep_X = test_X[start_num:end_num]\n",
    "    sep_Y = test_cate_Y[start_num:end_num]\n",
    "    sep_press_Y = test_Y[:,0][start_num:end_num]\n",
    "    real_label = i//number_of_group\n",
    "    \n",
    "    df = pd.DataFrame(sep_press_Y)\n",
    "    sep_press_cate_Y = df.apply(lambda x : pd.cut(x, cut_list,labels=[2,1,0]))\n",
    "    \n",
    "    group0_sep_index = np.where(sep_press_cate_Y==0)[0]\n",
    "    group1_sep_index = np.where(sep_press_cate_Y==1)[0]\n",
    "    group2_sep_index = np.where(sep_press_cate_Y==2)[0]\n",
    "    \n",
    "    group0_sep_X = sep_X[group0_sep_index]\n",
    "    group0_sep_Y = sep_Y[group0_sep_index]\n",
    "    group1_sep_X = sep_X[group1_sep_index]\n",
    "    group1_sep_Y = sep_Y[group1_sep_index]\n",
    "    group2_sep_X = sep_X[group2_sep_index]\n",
    "    group2_sep_Y = sep_Y[group2_sep_index]\n",
    "    \n",
    "    score, acc_all =  cate_model.evaluate(sep_X, sep_Y, verbose =0)\n",
    "    score, acc_0 =  cate_model.evaluate(group0_sep_X, group0_sep_Y, verbose =0)\n",
    "    score, acc_1 =  cate_model.evaluate(group1_sep_X, group1_sep_Y, verbose =0)\n",
    "    score, acc_2 =  cate_model.evaluate(group2_sep_X, group2_sep_Y, verbose =0)\n",
    "    \n",
    "    y_proba = cate_model.predict(sep_X)\n",
    "    all_y_classes = y_proba.argmax(axis=-1)\n",
    "    group0_y_classes = all_y_classes[group0_sep_index]\n",
    "    group1_y_classes = all_y_classes[group1_sep_index]\n",
    "    group2_y_classes = all_y_classes[group2_sep_index]\n",
    "    \n",
    "    counts = np.bincount(all_y_classes)\n",
    "    all_freq_label = np.argmax(counts)\n",
    "    all_right = all_freq_label == real_label\n",
    "    \n",
    "    counts = np.bincount(group0_y_classes)\n",
    "    group0_freq_label = np.argmax(counts)\n",
    "    group0_right = group0_freq_label == real_label\n",
    "    \n",
    "    counts = np.bincount(group1_y_classes)\n",
    "    group1_freq_label = np.argmax(counts)\n",
    "    group1_right = group1_freq_label == real_label\n",
    "    \n",
    "    counts = np.bincount(group2_y_classes)\n",
    "    group2_freq_label = np.argmax(counts)\n",
    "    group2_right = group2_freq_label == real_label\n",
    "    \n",
    "    \n",
    "    sep_result = (real_label, i%number_of_group, acc_all, acc_0, acc_1, acc_2, all_freq_label, all_right,\\\n",
    "                  group0_freq_label, group0_right, group1_freq_label, group1_right, group2_freq_label, group2_right)\n",
    "\n",
    "    result.append(sep_result)\n",
    "    print(result)\n",
    "#     print(sep_Y.shape)\n",
    "    \n",
    "    if i%number_of_group ==0:\n",
    "        print(real_label)\n",
    "\n",
    "test_result = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "cut_list =[-1,-0.8,-0.6,0]\n",
    "filepath='D:/AI_data_set/Patch/One_Pad_Model&Data/180118_One_Pad_cate.out'\n",
    "cate_model.load_weights(filepath) \n",
    "\n",
    "number_of_group =20\n",
    "number_of_label =60\n",
    "\n",
    "for i in range(0, number_of_group*number_of_label):\n",
    "    \n",
    "    start_num = 800*(i)\n",
    "    end_num = 800*(i+1)\n",
    "    sep_X = train_X[start_num:end_num]\n",
    "    sep_Y = train_cate_Y[start_num:end_num]\n",
    "    sep_press_Y = train_Y[:,0][start_num:end_num]\n",
    "    real_label = i//number_of_group\n",
    "    \n",
    "    df = pd.DataFrame(sep_press_Y)\n",
    "    sep_press_cate_Y = df.apply(lambda x : pd.cut(x, cut_list,labels=[2,1,0]))\n",
    "    \n",
    "    group0_sep_index = np.where(sep_press_cate_Y==0)[0]\n",
    "    group1_sep_index = np.where(sep_press_cate_Y==1)[0]\n",
    "    group2_sep_index = np.where(sep_press_cate_Y==2)[0]\n",
    "    \n",
    "    group0_sep_X = sep_X[group0_sep_index]\n",
    "    group0_sep_Y = sep_Y[group0_sep_index]\n",
    "    group1_sep_X = sep_X[group1_sep_index]\n",
    "    group1_sep_Y = sep_Y[group1_sep_index]\n",
    "    group2_sep_X = sep_X[group2_sep_index]\n",
    "    group2_sep_Y = sep_Y[group2_sep_index]\n",
    "    \n",
    "    score, acc_all =  cate_model.evaluate(sep_X, sep_Y, verbose =0)\n",
    "    score, acc_0 =  cate_model.evaluate(group0_sep_X, group0_sep_Y, verbose =0)\n",
    "    score, acc_1 =  cate_model.evaluate(group1_sep_X, group1_sep_Y, verbose =0)\n",
    "    score, acc_2 =  cate_model.evaluate(group2_sep_X, group2_sep_Y, verbose =0)\n",
    "    \n",
    "    y_proba = cate_model.predict(sep_X)\n",
    "    all_y_classes = y_proba.argmax(axis=-1)\n",
    "    group0_y_classes = all_y_classes[group0_sep_index]\n",
    "    group1_y_classes = all_y_classes[group1_sep_index]\n",
    "    group2_y_classes = all_y_classes[group2_sep_index]\n",
    "    \n",
    "    counts = np.bincount(all_y_classes)\n",
    "    all_freq_label = np.argmax(counts)\n",
    "    all_right = all_freq_label == real_label\n",
    "    \n",
    "    counts = np.bincount(group0_y_classes)\n",
    "    group0_freq_label = np.argmax(counts)\n",
    "    group0_right = group0_freq_label == real_label\n",
    "    \n",
    "    counts = np.bincount(group1_y_classes)\n",
    "    group1_freq_label = np.argmax(counts)\n",
    "    group1_right = group1_freq_label == real_label\n",
    "    \n",
    "    counts = np.bincount(group2_y_classes)\n",
    "    group2_freq_label = np.argmax(counts)\n",
    "    group2_right = group2_freq_label == real_label\n",
    "    \n",
    "    sep_result = (real_label, i%number_of_group, acc_all, acc_0, acc_1, acc_2, all_freq_label, all_right,\\\n",
    "                  group0_freq_label, group0_right, group1_freq_label, group1_right, group2_freq_label, group2_right)\n",
    "\n",
    "    result.append(sep_result)\n",
    "    print()\n",
    "    \n",
    "    if i%number_of_group ==0:\n",
    "        print(real_label)\n",
    "\n",
    "train_result = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1         2         3         4         5     6    7     8    9     10  \\\n",
      "0                                                                               \n",
      "0.0   9.5  0.673250  0.438980  1.000000  1.000000   0.0  1.0   0.0  1.0   0.0   \n",
      "1.0   9.5  0.624750  0.363407  0.981059  1.000000   1.0  1.0   1.0  1.0   1.0   \n",
      "2.0   9.5  0.706187  0.488060  1.000000  1.000000   2.0  1.0   2.0  1.0   2.0   \n",
      "3.0   9.5  0.771188  0.602699  1.000000  1.000000   3.0  1.0   3.0  1.0   3.0   \n",
      "4.0   9.5  0.773250  0.605184  1.000000  1.000000   4.0  1.0   4.0  1.0   4.0   \n",
      "5.0   9.5  0.768250  0.606340  1.000000  1.000000   5.0  1.0   5.0  1.0   5.0   \n",
      "6.0   9.5  0.768000  0.587917  1.000000  1.000000   6.0  1.0   6.0  1.0   6.0   \n",
      "7.0   9.5  0.735875  0.581479  1.000000  1.000000   7.0  1.0   7.0  1.0   7.0   \n",
      "8.0   9.5  0.820813  0.694664  1.000000  1.000000   8.0  1.0   8.0  1.0   8.0   \n",
      "9.0   9.5  0.777062  0.599565  1.000000  1.000000   9.0  1.0   9.0  1.0   9.0   \n",
      "10.0  9.5  0.690688  0.452310  0.994499  1.000000  10.0  1.0  10.0  1.0  10.0   \n",
      "11.0  9.5  0.734625  0.531610  1.000000  1.000000  11.0  1.0  11.0  1.0  11.0   \n",
      "12.0  9.5  0.651875  0.411382  0.998313  1.000000  12.0  1.0  12.0  1.0  12.0   \n",
      "13.0  9.5  0.613437  0.341579  0.989056  1.000000  13.0  1.0  13.0  1.0  13.0   \n",
      "14.0  9.5  0.583688  0.281551  0.966674  0.999706  14.0  1.0  14.0  1.0  14.0   \n",
      "15.0  9.5  0.650750  0.397743  0.998257  1.000000  15.0  1.0  15.0  1.0  15.0   \n",
      "16.0  9.5  0.635688  0.372277  0.995429  1.000000  16.0  1.0  16.0  1.0  16.0   \n",
      "17.0  9.5  0.705063  0.501447  0.999716  1.000000  17.0  1.0  17.0  1.0  17.0   \n",
      "18.0  9.5  0.789438  0.644371  1.000000  1.000000  18.0  1.0  18.0  1.0  18.0   \n",
      "19.0  9.5  0.679062  0.462315  0.998611  1.000000  19.0  1.0  19.0  1.0  19.0   \n",
      "20.0  9.5  0.712625  0.489973  1.000000  1.000000  20.0  1.0  20.0  1.0  20.0   \n",
      "21.0  9.5  0.613500  0.323973  0.998075  1.000000  21.0  1.0  21.0  1.0  21.0   \n",
      "22.0  9.5  0.693063  0.441568  0.996433  1.000000  22.0  1.0  22.0  1.0  22.0   \n",
      "23.0  9.5  0.717938  0.484289  1.000000  1.000000  23.0  1.0  23.0  1.0  23.0   \n",
      "24.0  9.5  0.657813  0.422769  0.999719  1.000000  24.0  1.0  24.0  1.0  24.0   \n",
      "25.0  9.5  0.638250  0.392582  0.971459  1.000000  25.0  1.0  25.0  1.0  25.0   \n",
      "26.0  9.5  0.566250  0.281938  0.950575  0.998508  26.0  1.0  26.0  1.0  26.0   \n",
      "27.0  9.5  0.581375  0.279423  0.977207  0.999482  27.0  1.0  27.0  1.0  27.0   \n",
      "28.0  9.5  0.661563  0.414496  0.999076  1.000000  28.0  1.0  28.0  1.0  28.0   \n",
      "29.0  9.5  0.794125  0.651898  1.000000  1.000000  29.0  1.0  29.0  1.0  29.0   \n",
      "30.0  9.5  0.865563  0.768894  1.000000  1.000000  30.0  1.0  30.0  1.0  30.0   \n",
      "31.0  9.5  0.652875  0.407229  0.998857  1.000000  31.0  1.0  31.0  1.0  31.0   \n",
      "32.0  9.5  0.607563  0.309418  0.994247  1.000000  32.0  1.0  32.0  1.0  32.0   \n",
      "33.0  9.5  0.616562  0.316845  0.979571  0.999542  33.0  1.0  33.0  1.0  33.0   \n",
      "34.0  9.5  0.608500  0.305333  0.978141  1.000000  34.0  1.0  34.0  1.0  34.0   \n",
      "35.0  9.5  0.686438  0.431698  1.000000  1.000000  35.0  1.0  35.0  1.0  35.0   \n",
      "36.0  9.5  0.667688  0.436919  1.000000  1.000000  36.0  1.0  36.0  1.0  36.0   \n",
      "37.0  9.5  0.575438  0.287041  0.977739  1.000000  37.0  1.0  37.0  1.0  37.0   \n",
      "38.0  9.5  0.607938  0.313604  0.982161  1.000000  38.0  1.0  38.0  1.0  38.0   \n",
      "39.0  9.5  0.647938  0.396821  0.999437  1.000000  39.0  1.0  39.0  1.0  39.0   \n",
      "40.0  9.5  0.673000  0.435718  1.000000  1.000000  40.0  1.0  40.0  1.0  40.0   \n",
      "41.0  9.5  0.798500  0.656490  1.000000  1.000000  41.0  1.0  41.0  1.0  41.0   \n",
      "42.0  9.5  0.741000  0.555647  1.000000  1.000000  42.0  1.0  42.0  1.0  42.0   \n",
      "43.0  9.5  0.726562  0.529161  1.000000  1.000000  43.0  1.0  43.0  1.0  43.0   \n",
      "44.0  9.5  0.721375  0.492435  1.000000  1.000000  44.0  1.0  44.0  1.0  44.0   \n",
      "45.0  9.5  0.657500  0.382416  0.995623  1.000000  45.0  1.0  45.0  1.0  45.0   \n",
      "46.0  9.5  0.675563  0.424599  0.990062  0.999305  46.0  1.0  46.0  1.0  46.0   \n",
      "47.0  9.5  0.709063  0.488865  1.000000  1.000000  47.0  1.0  47.0  1.0  47.0   \n",
      "48.0  9.5  0.705125  0.503209  1.000000  1.000000  48.0  1.0  48.0  1.0  48.0   \n",
      "49.0  9.5  0.680000  0.434442  0.995108  0.999733  49.0  1.0  49.0  1.0  49.0   \n",
      "50.0  9.5  0.736563  0.557504  1.000000  1.000000  50.0  1.0  50.0  1.0  50.0   \n",
      "51.0  9.5  0.807312  0.676947  1.000000  1.000000  51.0  1.0  51.0  1.0  51.0   \n",
      "52.0  9.5  0.843750  0.734045  1.000000  1.000000  52.0  1.0  52.0  1.0  52.0   \n",
      "53.0  9.5  0.812875  0.680429  1.000000  1.000000  53.0  1.0  53.0  1.0  53.0   \n",
      "54.0  9.5  0.822187  0.707573  1.000000  1.000000  54.0  1.0  54.0  1.0  54.0   \n",
      "55.0  9.5  0.756812  0.595424  1.000000  1.000000  55.0  1.0  55.0  1.0  55.0   \n",
      "56.0  9.5  0.781375  0.631459  1.000000  1.000000  56.0  1.0  56.0  1.0  56.0   \n",
      "57.0  9.5  0.792563  0.630945  1.000000  1.000000  57.0  1.0  57.0  1.0  57.0   \n",
      "58.0  9.5  0.712875  0.500884  0.999706  1.000000  58.0  1.0  58.0  1.0  58.0   \n",
      "59.0  9.5  0.732125  0.520827  1.000000  1.000000  59.0  1.0  59.0  1.0  59.0   \n",
      "\n",
      "       11    12   13  \n",
      "0                     \n",
      "0.0   1.0   0.0  1.0  \n",
      "1.0   1.0   1.0  1.0  \n",
      "2.0   1.0   2.0  1.0  \n",
      "3.0   1.0   3.0  1.0  \n",
      "4.0   1.0   4.0  1.0  \n",
      "5.0   1.0   5.0  1.0  \n",
      "6.0   1.0   6.0  1.0  \n",
      "7.0   1.0   7.0  1.0  \n",
      "8.0   1.0   8.0  1.0  \n",
      "9.0   1.0   9.0  1.0  \n",
      "10.0  1.0  10.0  1.0  \n",
      "11.0  1.0  11.0  1.0  \n",
      "12.0  1.0  12.0  1.0  \n",
      "13.0  1.0  13.0  1.0  \n",
      "14.0  1.0  14.0  1.0  \n",
      "15.0  1.0  15.0  1.0  \n",
      "16.0  1.0  16.0  1.0  \n",
      "17.0  1.0  17.0  1.0  \n",
      "18.0  1.0  18.0  1.0  \n",
      "19.0  1.0  19.0  1.0  \n",
      "20.0  1.0  20.0  1.0  \n",
      "21.0  1.0  21.0  1.0  \n",
      "22.0  1.0  22.0  1.0  \n",
      "23.0  1.0  23.0  1.0  \n",
      "24.0  1.0  24.0  1.0  \n",
      "25.0  1.0  25.0  1.0  \n",
      "26.0  1.0  26.0  1.0  \n",
      "27.0  1.0  27.0  1.0  \n",
      "28.0  1.0  28.0  1.0  \n",
      "29.0  1.0  29.0  1.0  \n",
      "30.0  1.0  30.0  1.0  \n",
      "31.0  1.0  31.0  1.0  \n",
      "32.0  1.0  32.0  1.0  \n",
      "33.0  1.0  33.0  1.0  \n",
      "34.0  1.0  34.0  1.0  \n",
      "35.0  1.0  35.0  1.0  \n",
      "36.0  1.0  36.0  1.0  \n",
      "37.0  1.0  37.0  1.0  \n",
      "38.0  1.0  38.0  1.0  \n",
      "39.0  1.0  39.0  1.0  \n",
      "40.0  1.0  40.0  1.0  \n",
      "41.0  1.0  41.0  1.0  \n",
      "42.0  1.0  42.0  1.0  \n",
      "43.0  1.0  43.0  1.0  \n",
      "44.0  1.0  44.0  1.0  \n",
      "45.0  1.0  45.0  1.0  \n",
      "46.0  1.0  46.0  1.0  \n",
      "47.0  1.0  47.0  1.0  \n",
      "48.0  1.0  48.0  1.0  \n",
      "49.0  1.0  49.0  1.0  \n",
      "50.0  1.0  50.0  1.0  \n",
      "51.0  1.0  51.0  1.0  \n",
      "52.0  1.0  52.0  1.0  \n",
      "53.0  1.0  53.0  1.0  \n",
      "54.0  1.0  54.0  1.0  \n",
      "55.0  1.0  55.0  1.0  \n",
      "56.0  1.0  56.0  1.0  \n",
      "57.0  1.0  57.0  1.0  \n",
      "58.0  1.0  58.0  1.0  \n",
      "59.0  1.0  59.0  1.0  \n",
      "         1         2          3          4          5       6     7       8   \\\n",
      "0                                                                              \n",
      "0.0   190.0  13.46500   8.779600  20.000000  20.000000     0.0  20.0     0.0   \n",
      "1.0   190.0  12.49500   7.268148  19.621177  20.000000    20.0  20.0    20.0   \n",
      "2.0   190.0  14.12375   9.761195  20.000000  20.000000    40.0  20.0    40.0   \n",
      "3.0   190.0  15.42375  12.053985  20.000000  20.000000    60.0  20.0    60.0   \n",
      "4.0   190.0  15.46500  12.103677  20.000000  20.000000    80.0  20.0    80.0   \n",
      "5.0   190.0  15.36500  12.126800  20.000000  20.000000   100.0  20.0   100.0   \n",
      "6.0   190.0  15.36000  11.758333  20.000000  20.000000   120.0  20.0   120.0   \n",
      "7.0   190.0  14.71750  11.629577  20.000000  20.000000   140.0  20.0   140.0   \n",
      "8.0   190.0  16.41625  13.893275  20.000000  20.000000   160.0  20.0   160.0   \n",
      "9.0   190.0  15.54125  11.991296  20.000000  20.000000   180.0  20.0   180.0   \n",
      "10.0  190.0  13.81375   9.046195  19.889987  20.000000   200.0  20.0   200.0   \n",
      "11.0  190.0  14.69250  10.632190  20.000000  20.000000   220.0  20.0   220.0   \n",
      "12.0  190.0  13.03750   8.227647  19.966260  20.000000   240.0  20.0   240.0   \n",
      "13.0  190.0  12.26875   6.831574  19.781115  20.000000   260.0  20.0   260.0   \n",
      "14.0  190.0  11.67375   5.631015  19.333473  19.994118   280.0  20.0   280.0   \n",
      "15.0  190.0  13.01500   7.954868  19.965149  20.000000   300.0  20.0   300.0   \n",
      "16.0  190.0  12.71375   7.445538  19.908572  20.000000   320.0  20.0   320.0   \n",
      "17.0  190.0  14.10125  10.028939  19.994318  20.000000   340.0  20.0   340.0   \n",
      "18.0  190.0  15.78875  12.887414  20.000000  20.000000   360.0  20.0   360.0   \n",
      "19.0  190.0  13.58125   9.246294  19.972222  20.000000   380.0  20.0   380.0   \n",
      "20.0  190.0  14.25250   9.799468  20.000000  20.000000   400.0  20.0   400.0   \n",
      "21.0  190.0  12.27000   6.479466  19.961496  20.000000   420.0  20.0   420.0   \n",
      "22.0  190.0  13.86125   8.831363  19.928655  20.000000   440.0  20.0   440.0   \n",
      "23.0  190.0  14.35875   9.685784  20.000000  20.000000   460.0  20.0   460.0   \n",
      "24.0  190.0  13.15625   8.455376  19.994382  20.000000   480.0  20.0   480.0   \n",
      "25.0  190.0  12.76500   7.851633  19.429185  20.000000   500.0  20.0   500.0   \n",
      "26.0  190.0  11.32500   5.638755  19.011502  19.970165   520.0  20.0   520.0   \n",
      "27.0  190.0  11.62750   5.588465  19.544145  19.989630   540.0  20.0   540.0   \n",
      "28.0  190.0  13.23125   8.289916  19.981518  20.000000   560.0  20.0   560.0   \n",
      "29.0  190.0  15.88250  13.037958  20.000000  20.000000   580.0  20.0   580.0   \n",
      "30.0  190.0  17.31125  15.377873  20.000000  20.000000   600.0  20.0   600.0   \n",
      "31.0  190.0  13.05750   8.144572  19.977143  20.000000   620.0  20.0   620.0   \n",
      "32.0  190.0  12.15125   6.188362  19.884941  20.000000   640.0  20.0   640.0   \n",
      "33.0  190.0  12.33125   6.336902  19.591412  19.990848   660.0  20.0   660.0   \n",
      "34.0  190.0  12.17000   6.106654  19.562818  20.000000   680.0  20.0   680.0   \n",
      "35.0  190.0  13.72875   8.633953  20.000000  20.000000   700.0  20.0   700.0   \n",
      "36.0  190.0  13.35375   8.738389  20.000000  20.000000   720.0  20.0   720.0   \n",
      "37.0  190.0  11.50875   5.740812  19.554772  20.000000   740.0  20.0   740.0   \n",
      "38.0  190.0  12.15875   6.272089  19.643219  20.000000   760.0  20.0   760.0   \n",
      "39.0  190.0  12.95875   7.936425  19.988732  20.000000   780.0  20.0   780.0   \n",
      "40.0  190.0  13.46000   8.714360  20.000000  20.000000   800.0  20.0   800.0   \n",
      "41.0  190.0  15.97000  13.129806  20.000000  20.000000   820.0  20.0   820.0   \n",
      "42.0  190.0  14.82000  11.112948  20.000000  20.000000   840.0  20.0   840.0   \n",
      "43.0  190.0  14.53125  10.583215  20.000000  20.000000   860.0  20.0   860.0   \n",
      "44.0  190.0  14.42750   9.848695  20.000000  20.000000   880.0  20.0   880.0   \n",
      "45.0  190.0  13.15000   7.648327  19.912459  20.000000   900.0  20.0   900.0   \n",
      "46.0  190.0  13.51125   8.491976  19.801238  19.986099   920.0  20.0   920.0   \n",
      "47.0  190.0  14.18125   9.777296  20.000000  20.000000   940.0  20.0   940.0   \n",
      "48.0  190.0  14.10250  10.064186  20.000000  20.000000   960.0  20.0   960.0   \n",
      "49.0  190.0  13.60000   8.688846  19.902165  19.994652   980.0  20.0   980.0   \n",
      "50.0  190.0  14.73125  11.150077  20.000000  20.000000  1000.0  20.0  1000.0   \n",
      "51.0  190.0  16.14625  13.538950  20.000000  20.000000  1020.0  20.0  1020.0   \n",
      "52.0  190.0  16.87500  14.680902  20.000000  20.000000  1040.0  20.0  1040.0   \n",
      "53.0  190.0  16.25750  13.608575  20.000000  20.000000  1060.0  20.0  1060.0   \n",
      "54.0  190.0  16.44375  14.151452  20.000000  20.000000  1080.0  20.0  1080.0   \n",
      "55.0  190.0  15.13625  11.908478  20.000000  20.000000  1100.0  20.0  1100.0   \n",
      "56.0  190.0  15.62750  12.629180  20.000000  20.000000  1120.0  20.0  1120.0   \n",
      "57.0  190.0  15.85125  12.618900  20.000000  20.000000  1140.0  20.0  1140.0   \n",
      "58.0  190.0  14.25750  10.017671  19.994118  20.000000  1160.0  20.0  1160.0   \n",
      "59.0  190.0  14.64250  10.416546  20.000000  20.000000  1180.0  20.0  1180.0   \n",
      "\n",
      "        9       10    11      12    13  \n",
      "0                                       \n",
      "0.0   20.0     0.0  20.0     0.0  20.0  \n",
      "1.0   20.0    20.0  20.0    20.0  20.0  \n",
      "2.0   20.0    40.0  20.0    40.0  20.0  \n",
      "3.0   20.0    60.0  20.0    60.0  20.0  \n",
      "4.0   20.0    80.0  20.0    80.0  20.0  \n",
      "5.0   20.0   100.0  20.0   100.0  20.0  \n",
      "6.0   20.0   120.0  20.0   120.0  20.0  \n",
      "7.0   20.0   140.0  20.0   140.0  20.0  \n",
      "8.0   20.0   160.0  20.0   160.0  20.0  \n",
      "9.0   20.0   180.0  20.0   180.0  20.0  \n",
      "10.0  20.0   200.0  20.0   200.0  20.0  \n",
      "11.0  20.0   220.0  20.0   220.0  20.0  \n",
      "12.0  20.0   240.0  20.0   240.0  20.0  \n",
      "13.0  20.0   260.0  20.0   260.0  20.0  \n",
      "14.0  20.0   280.0  20.0   280.0  20.0  \n",
      "15.0  20.0   300.0  20.0   300.0  20.0  \n",
      "16.0  20.0   320.0  20.0   320.0  20.0  \n",
      "17.0  20.0   340.0  20.0   340.0  20.0  \n",
      "18.0  20.0   360.0  20.0   360.0  20.0  \n",
      "19.0  20.0   380.0  20.0   380.0  20.0  \n",
      "20.0  20.0   400.0  20.0   400.0  20.0  \n",
      "21.0  20.0   420.0  20.0   420.0  20.0  \n",
      "22.0  20.0   440.0  20.0   440.0  20.0  \n",
      "23.0  20.0   460.0  20.0   460.0  20.0  \n",
      "24.0  20.0   480.0  20.0   480.0  20.0  \n",
      "25.0  20.0   500.0  20.0   500.0  20.0  \n",
      "26.0  20.0   520.0  20.0   520.0  20.0  \n",
      "27.0  20.0   540.0  20.0   540.0  20.0  \n",
      "28.0  20.0   560.0  20.0   560.0  20.0  \n",
      "29.0  20.0   580.0  20.0   580.0  20.0  \n",
      "30.0  20.0   600.0  20.0   600.0  20.0  \n",
      "31.0  20.0   620.0  20.0   620.0  20.0  \n",
      "32.0  20.0   640.0  20.0   640.0  20.0  \n",
      "33.0  20.0   660.0  20.0   660.0  20.0  \n",
      "34.0  20.0   680.0  20.0   680.0  20.0  \n",
      "35.0  20.0   700.0  20.0   700.0  20.0  \n",
      "36.0  20.0   720.0  20.0   720.0  20.0  \n",
      "37.0  20.0   740.0  20.0   740.0  20.0  \n",
      "38.0  20.0   760.0  20.0   760.0  20.0  \n",
      "39.0  20.0   780.0  20.0   780.0  20.0  \n",
      "40.0  20.0   800.0  20.0   800.0  20.0  \n",
      "41.0  20.0   820.0  20.0   820.0  20.0  \n",
      "42.0  20.0   840.0  20.0   840.0  20.0  \n",
      "43.0  20.0   860.0  20.0   860.0  20.0  \n",
      "44.0  20.0   880.0  20.0   880.0  20.0  \n",
      "45.0  20.0   900.0  20.0   900.0  20.0  \n",
      "46.0  20.0   920.0  20.0   920.0  20.0  \n",
      "47.0  20.0   940.0  20.0   940.0  20.0  \n",
      "48.0  20.0   960.0  20.0   960.0  20.0  \n",
      "49.0  20.0   980.0  20.0   980.0  20.0  \n",
      "50.0  20.0  1000.0  20.0  1000.0  20.0  \n",
      "51.0  20.0  1020.0  20.0  1020.0  20.0  \n",
      "52.0  20.0  1040.0  20.0  1040.0  20.0  \n",
      "53.0  20.0  1060.0  20.0  1060.0  20.0  \n",
      "54.0  20.0  1080.0  20.0  1080.0  20.0  \n",
      "55.0  20.0  1100.0  20.0  1100.0  20.0  \n",
      "56.0  20.0  1120.0  20.0  1120.0  20.0  \n",
      "57.0  20.0  1140.0  20.0  1140.0  20.0  \n",
      "58.0  20.0  1160.0  20.0  1160.0  20.0  \n",
      "59.0  20.0  1180.0  20.0  1180.0  20.0  \n"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame(test_result)\n",
    "# test_mean = df.groupby([0]).mean()\n",
    "\n",
    "# df = pd.DataFrame(test_result)\n",
    "# test_sum = df.groupby([0]).sum()\n",
    "\n",
    "df = pd.DataFrame(train_result)\n",
    "train_mean = df.groupby([0]).mean()\n",
    "\n",
    "\n",
    "df = pd.DataFrame(train_result)\n",
    "train_sum = df.groupby([0]).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
